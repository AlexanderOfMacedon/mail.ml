{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from IPython.display import Image, SVG\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('C:\\\\Users\\\\Xiaomi\\\\JN\\\\mail.ru\\\\homework4\\\\cosmo_content_storage_final_cut.jsonl', 'rb') as json_file:\n",
    "    json_list_data = [eval(line.decode(\"utf-8\"))  for line in json_file.readlines()]\n",
    "with open('C:\\\\Users\\\\Xiaomi\\\\JN\\\\mail.ru\\\\homework4\\\\cluster_final_cut_train.json', 'rb') as json_file:\n",
    "    json_list_target = [eval(line.decode(\"utf-8\"))  for line in json_file.readlines()]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>ts</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>У России есть необходимые конкурентные преимущ...</td>\n",
       "      <td>1000029981939875422</td>\n",
       "      <td>Новак заявил о способности России стать лидеро...</td>\n",
       "      <td>1575625264</td>\n",
       "      <td>https://vz.ru/news/2019/12/6/1012187.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>По делу об убийстве главы ингушского центра «Э...</td>\n",
       "      <td>1000103932845397396</td>\n",
       "      <td>СК отчитался о поимке банды, причастной к убий...</td>\n",
       "      <td>1575634419</td>\n",
       "      <td>https://meduza.io/news/2019/12/06/sk-otchitals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Президент Азербайджана Ильхам Алиев и первый в...</td>\n",
       "      <td>1000115462666782749</td>\n",
       "      <td>Ильхам Алиев и Мехрибан Алиева посетили выстав...</td>\n",
       "      <td>1575447101</td>\n",
       "      <td>http://www.vestikavkaza.ru/material/283355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Проблемы и вызовы языковой политики анализируе...</td>\n",
       "      <td>1000129039443894284</td>\n",
       "      <td>Язык и идеология</td>\n",
       "      <td>1575767444</td>\n",
       "      <td>http://krizis-kopilka.ru/archives/70325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Отношения России и Турции не являются альтерна...</td>\n",
       "      <td>1000131983536261699</td>\n",
       "      <td>Эрдоган: отношения Турции с РФ не альтернативн...</td>\n",
       "      <td>1575359865</td>\n",
       "      <td>https://radiosputnik.ria.ru/20191203/156189583...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description               doc_id  \\\n",
       "0  У России есть необходимые конкурентные преимущ...  1000029981939875422   \n",
       "1  По делу об убийстве главы ингушского центра «Э...  1000103932845397396   \n",
       "2  Президент Азербайджана Ильхам Алиев и первый в...  1000115462666782749   \n",
       "3  Проблемы и вызовы языковой политики анализируе...  1000129039443894284   \n",
       "4  Отношения России и Турции не являются альтерна...  1000131983536261699   \n",
       "\n",
       "                                               title          ts  \\\n",
       "0  Новак заявил о способности России стать лидеро...  1575625264   \n",
       "1  СК отчитался о поимке банды, причастной к убий...  1575634419   \n",
       "2  Ильхам Алиев и Мехрибан Алиева посетили выстав...  1575447101   \n",
       "3                                   Язык и идеология  1575767444   \n",
       "4  Эрдоган: отношения Турции с РФ не альтернативн...  1575359865   \n",
       "\n",
       "                                                 url  \n",
       "0          https://vz.ru/news/2019/12/6/1012187.html  \n",
       "1  https://meduza.io/news/2019/12/06/sk-otchitals...  \n",
       "2         http://www.vestikavkaza.ru/material/283355  \n",
       "3            http://krizis-kopilka.ru/archives/70325  \n",
       "4  https://radiosputnik.ria.ru/20191203/156189583...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullData = pd.DataFrame(json_list_data)\n",
    "fullTargetData = pd.DataFrame(json_list_target)\n",
    "fullTextData = fullData.drop(columns=['ts', 'url'])\n",
    "fullTextData['text'] = fullTextData.description + \" \" + fullTextData.title\n",
    "del fullTextData['description']\n",
    "del fullTextData['title']\n",
    "# textData = fullTextData.drop(columns=['description'])\n",
    "# descData = fullTextData.drop(columns=['title'])\n",
    "fullData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формирование marked и unmarked датасетов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "markDataFull = pd.DataFrame(columns=list(fullTextData.columns) + ['mark'])\n",
    "for id in fullTargetData.columns:\n",
    "    t = fullTextData.loc[fullTextData['doc_id'] == int(id)].iloc[0]\n",
    "    t1 = t.append(pd.Series([fullTargetData[id].iloc[0]], index=[\"mark\"]))\n",
    "    markDataFull.loc[t.name] = t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexMarkedData = markDataFull.index.values.tolist()\n",
    "indexAllData = fullTextData.index.values.tolist()\n",
    "indexUnmarkedData = [item for item in indexAllData if item not in indexMarkedData]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmarkedData = fullTextData.iloc[indexUnmarkedData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xiaomi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "nans = fullTextData.text[fullTextData.text.isna()]\n",
    "for index in nans.index:\n",
    "    fullTextData.text[index] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "import string\n",
    "stemmerRus = SnowballStemmer(\"russian\") \n",
    "stemmerEng = SnowballStemmer(\"english\") \n",
    "stop_words = set(stopwords.words('russian'))\n",
    "stop_words = stop_words.union(set(stopwords.words('english')))\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "def preprocessing(text):\n",
    "    temp = text.lower()\n",
    "    temp = re.sub(r'\\d+', '', temp)\n",
    "    temp = temp.translate(str.maketrans('','', string.punctuation)).strip()\n",
    "    tokens = word_tokenize(temp)\n",
    "    temp = [wl.lemmatize(i) if not bool(re.search('[а-яА-Я]', i)) else stemmerRus.stem(i) for i in tokens if not i in stop_words]\n",
    "    temp = [word if len(word)>2 else '' for word in temp]\n",
    "    return ' '.join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'жил бабус hello book весел  бел сер'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(\"жили у бабуси hello books a ? - [ ] . 123 два веселых гуся \\\"  один белый другой серый\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function preprocessing at 0x000001B924D8CD08>,\n",
       "        smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "        sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(preprocessor=preprocessing)\n",
    "vectorizer.fit(fullTextData.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88368, 75869)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullTextData.text = [preprocessing(text) for text in fullTextData.text]\n",
    "matrixFullText = vectorizer.transform(fullTextData.text)\n",
    "matrixFullText.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xiaomi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Xiaomi\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3296: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\Xiaomi\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5096: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(61858, 75869)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nans = unmarkedData.text[unmarkedData.text.isna()]\n",
    "for index in nans.index:\n",
    "    unmarkedData.text[index] = \"\"\n",
    "unmarkedData.text = [preprocessing(text) for text in unmarkedData.text]\n",
    "matrixUnmarked = vectorizer.transform(unmarkedData.text)\n",
    "matrixUnmarked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26510, 75869)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nans = markDataFull.text[markDataFull.text.isna()]\n",
    "for index in nans.index:\n",
    "    markDataFull.text[index] = \"\"\n",
    "markDataFull.text = [preprocessing(text) for text in markDataFull.text]\n",
    "matrixTarget = vectorizer.transform(markDataFull.text)\n",
    "matrixTarget.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=4000, n_iter=5,\n",
       "       random_state=13, tol=0.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "svd = TruncatedSVD(n_components=4000, random_state=13)\n",
    "svd.fit(matrixFullText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88368, 4000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresFullText = svd.transform(matrixFullText)\n",
    "# matrix1 = matrix1.toarray()\n",
    "featuresFullText.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 4000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresClusterUnique = svd.transform(matrixClusterUnique)\n",
    "featuresClusterUnique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26510, 4000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresTarget = svd.transform(matrixTarget)\n",
    "featuresTarget.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61858, 4000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresUnmarked = svd.transform(matrixUnmarked)\n",
    "featuresUnmarked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попытка кластеризации (не получается метки сопоставить пока)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xiaomi\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:971: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True,\n",
       "    init=array([[ 0.01087, -0.00337, ...,  0.00094,  0.01007],\n",
       "       [ 0.02294, -0.00731, ...,  0.00911,  0.01238],\n",
       "       ...,\n",
       "       [ 0.00945, -0.00283, ...,  0.00055, -0.01124],\n",
       "       [ 0.03841, -0.01168, ..., -0.00266,  0.00247]]),\n",
       "    max_iter=300, n_clusters=3064, n_init=10, n_jobs=None,\n",
       "    precompute_distances='auto', random_state=46, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KMeans(init = feautures1, n_clusters=3064, random_state=46)\n",
    "model.fit(features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1596, 1119), (1846, 537), (2562, 112), (3034, 90), (2644, 61), (1221, 52), (1760, 49), (160, 47), (423, 45), (415, 44)]\n",
      "[(103, 399), (238, 190), (67, 178), (21, 171), (195, 167), (637, 157), (5, 140), (311, 131), (169, 130), (25, 118)]\n",
      "61858\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(subPreds).most_common(10))\n",
    "print(Counter(marks).most_common(10))\n",
    "print(len(indexUnmarkedData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "subPreds1 = [preds[index] for index in indexUnmarkedData]\n",
    "doc_ids = [fullData.doc_id[index] for index in indexUnmarkedData]\n",
    "with open(\"C:\\\\Users\\\\Xiaomi\\\\JN\\\\mail.ru\\\\homework4\\\\my_submission.txt\", 'w') as outFile:\n",
    "    outFile.write('doc_id,cat\\n')\n",
    "    for index in range(len(subPreds1)):\n",
    "        outFile.write(str(doc_ids[index]) +','+str(subPreds1[index]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попытка классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xiaomi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Xiaomi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Xiaomi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=8,\n",
       "          penalty='l2', random_state=None, solver='warn', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression( n_jobs=8)\n",
    "y = markDataFull.mark.astype ('int')\n",
    "model.fit(X=featuresTarget,y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(featuresUnmarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid = model.predict(featuresTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(type(pred_valid.tolist()))\n",
    "print(type(markDataFull.mark.to_numpy().tolist()))\n",
    "confusion_matrix(pred_valid.tolist(), markDataFull.mark.to_numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [fullData.doc_id[index] for index in indexUnmarkedData]\n",
    "with open(\"C:\\\\Users\\\\Xiaomi\\\\JN\\\\mail.ru\\\\homework4\\\\my_submission.txt\", 'w') as outFile:\n",
    "    outFile.write('doc_id,cat\\n')\n",
    "    for index in range(len(pred)):\n",
    "        outFile.write(str(doc_ids[index]) +','+str(pred[index]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 238,   57,  425, ..., 1235,  103,    6])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3059, 57, 425, ..., 1235, 1391, 335], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markDataFull.mark.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(markDataFull[markDataFull.mark == 6].text))\n",
    "\n",
    "mistakes = [1 if not pred_valid[index] == markDataFull.mark.values[index] else 0 for index in range(len(pred_valid))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43044134288947566"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mistakes) / len(pred_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Еще одна модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "modelNeigh = KNeighborsClassifier()\n",
    "modelNeigh.fit(X=featuresTarget,y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "predNeigh = modelNeigh.predict(featuresUnmarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "predValidNeigh = modelNeigh.predict(featuresTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakesNeigh = [1 if not predValidNeigh[index] == markDataFull.mark.values[index] else 0 for index in range(len(predValidNeigh))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1931723877781969"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mistakesNeigh) / len(predValidNeigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [fullData.doc_id[index] for index in indexUnmarkedData]\n",
    "with open(\"C:\\\\Users\\\\Xiaomi\\\\JN\\\\mail.ru\\\\homework4\\\\my_submission.txt\", 'w') as outFile:\n",
    "    outFile.write('doc_id,cat\\n')\n",
    "    for index in range(len(pred)):\n",
    "        outFile.write(str(doc_ids[index]) +','+str(predNeigh[index]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отбор признаков по feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xiaomi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "modelSelect = SelectFromModel(SGDClassifier(), max_features=12000)\n",
    "y = markDataFull.mark.astype ('int')\n",
    "selector = modelSelect.fit(X=matrixTarget, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26510, 10914)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newMatrixTarger = selector.transform(matrixTarget)\n",
    "newMatrixTarger.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "modelNB = MultinomialNB()\n",
    "y = markDataFull.mark.astype ('int')\n",
    "modelNB.fit(X=newMatrixTarger,y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7640890230101849"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predNB = modelNB.predict(selector.transform(matrixUnmarked))\n",
    "predValidNB = modelNB.predict(selector.transform(matrixTarget))\n",
    "mistakesNB = [1 if not predValidNB[index] == markDataFull.mark.values[index] else 0 for index in range(len(predValidNB))]\n",
    "sum(mistakesNB) / len(predValidNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "modelLog = SGDClassifier()\n",
    "modelLog.fit(X=newMatrixTarger,y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05322519803847605"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predLog = modelLog.predict(selector.transform(matrixUnmarked))\n",
    "predValidLog = modelLog.predict(newMatrixTarger)\n",
    "mistakesLog = [1 if not predValidLog[index] == markDataFull.mark.values[index] else 0 for index in range(len(predValidLog))]\n",
    "sum(mistakesLog) / len(predValidLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [fullData.doc_id[index] for index in indexUnmarkedData]\n",
    "with open(\"C:\\\\Users\\\\Xiaomi\\\\JN\\\\mail.ru\\\\homework4\\\\my_submission4.txt\", 'w') as outFile:\n",
    "    outFile.write('doc_id,cat\\n')\n",
    "    for index in range(len(predLog)):\n",
    "        outFile.write(str(doc_ids[index]) +','+str(predLog[index]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "modelLog = SGDClassifier()\n",
    "modelLog.fit(X=newMatrixTarger,y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLog = modelLog.predict(selector.transform(matrixUnmarked))\n",
    "predValidLog = modelLog.predict(newMatrixTarger)\n",
    "mistakesLog = [1 if not predValidLog[index] == markDataFull.mark.values[index] else 0 for index in range(len(predValidLog))]\n",
    "sum(mistakesLog) / len(predValidLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [fullData.doc_id[index] for index in indexUnmarkedData]\n",
    "with open(\"C:\\\\Users\\\\Xiaomi\\\\JN\\\\mail.ru\\\\homework4\\\\my_submission5.txt\", 'w') as outFile:\n",
    "    outFile.write('doc_id,cat\\n')\n",
    "    for index in range(len(predLog)):\n",
    "        outFile.write(str(doc_ids[index]) +','+str(predLog[index]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "modelKNN = KNeighborsClassifier(n_neighbors = 3)\n",
    "modelKNN.fit(X=newMatrixTarger,y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1955488494907582"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predKNN = modelKNN.predict(selector.transform(matrixUnmarked))\n",
    "predValidKNN = modelKNN.predict(selector.transform(matrixTarget))\n",
    "mistakesKNN = [1 if not predValidKNN[index] == markDataFull.mark.values[index] else 0 for index in range(len(predValidKNN))]\n",
    "sum(mistakesKNN) / len(predValidKNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "modelKNN1 = KNeighborsClassifier(n_neighbors = 5)\n",
    "modelKNN1.fit(X=newMatrixTarger,y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46035458317615996"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predKNN1 = modelKNN1.predict(selector.transform(matrixUnmarked))\n",
    "predValidKNN1 = modelKNN1.predict(selector.transform(matrixTarget))\n",
    "mistakesKNN1 = [1 if not predValidKNN1[index] == markDataFull.mark.values[index] else 0 for index in range(len(predValidKNN1))]\n",
    "sum(mistakesKNN1) / len(predValidKNN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Строим ансамбль моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"my_submission (1).txt\") as file1, open(\"my_submission (2).txt\") as file2, open(\"my_submission (3).txt\") as file3, open(\"my_submission4.txt\") as file4, open(\"my_submission5.txt\") as file5, open(\"my_submission_boost.txt\") as file6:\n",
    "    pred1 = file1.readlines()\n",
    "    del pred1[0]\n",
    "    for index in range(len(pred1)):\n",
    "        pred1[index] = int(pred1[index].split(',')[1].replace('\\n', ''))\n",
    "    pred2 = file2.readlines()\n",
    "    del pred2[0]\n",
    "    for index in range(len(pred2)):\n",
    "        pred2[index] = int(pred2[index].split(',')[1].replace('\\n', ''))\n",
    "    pred3 = file3.readlines()\n",
    "    del pred3[0]\n",
    "    for index in range(len(pred3)):\n",
    "        pred3[index] = int(pred3[index].split(',')[1].replace('\\n', ''))\n",
    "    pred4 = file4.readlines()\n",
    "    del pred4[0]\n",
    "    for index in range(len(pred4)):\n",
    "        pred4[index] = int(pred4[index].split(',')[1].replace('\\n', ''))\n",
    "    pred5 = file5.readlines()\n",
    "    del pred5[0]\n",
    "    for index in range(len(pred5)):\n",
    "        pred5[index] = int(pred5[index].split(',')[1].replace('\\n', ''))\n",
    "    pred6 = file6.readlines()\n",
    "    del pred6[0]\n",
    "    for index in range(len(pred6)):\n",
    "        pred6[index] = int(pred6[index].split(',')[1].replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "pred7 = predLog\n",
    "for index in range(len(pred7)):\n",
    "    counter = Counter([pred1[index], pred2[index], pred3[index], pred4[index], pred5[index], pred6[index]])\n",
    "    if counter.most_common(1)[0][1] >= 2:\n",
    "        pred7[index] = counter.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [fullData.doc_id[index] for index in indexUnmarkedData]\n",
    "with open(\"C:\\\\Users\\\\Xiaomi\\\\JN\\\\mail.ru\\\\homework4\\\\my_submission.txt\", 'w') as outFile:\n",
    "    outFile.write('doc_id,cat\\n')\n",
    "    for index in range(len(pred7)):\n",
    "        outFile.write(str(doc_ids[index]) +','+str(pred7[index]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xiaomi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=40, n_jobs=None, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "boostModel = BaggingClassifier(SGDClassifier(), n_estimators=40)\n",
    "boostModel.fit(X=newMatrixTarger,y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.061750282912108635"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predboost = boostModel.predict(selector.transform(matrixUnmarked))\n",
    "predValidboost = boostModel.predict(newMatrixTarger)\n",
    "mistakesboost = [1 if not predValidboost[index] == markDataFull.mark.values[index] else 0 for index in range(len(predValidboost))]\n",
    "sum(mistakesboost) / len(predValidboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [fullData.doc_id[index] for index in indexUnmarkedData]\n",
    "with open(\"C:\\\\Users\\\\Xiaomi\\\\JN\\\\mail.ru\\\\homework4\\\\my_submission_boost.txt\", 'w') as outFile:\n",
    "    outFile.write('doc_id,cat\\n')\n",
    "    for index in range(len(predboost)):\n",
    "        outFile.write(str(doc_ids[index]) +','+str(predboost[index]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
